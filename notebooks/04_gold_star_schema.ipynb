{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f39345f-eac9-4aa0-a807-8dfa12af5c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold - Star Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056ea4b1-733c-4c72-8783-f5c1391c0f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.0 - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f57d66-5642-4c88-a51e-4f30a56010c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119b1d60-1293-4e01-bd62-9579643bc8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.0 - Dimension: Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748e4643-5a8b-4a2c-8679-a892858d9f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id as location_id,\n",
    "        name as location_name,\n",
    "        locality,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        country_code,\n",
    "        country_name,\n",
    "        owner_name,\n",
    "        provider_name,\n",
    "        timezone\n",
    "    FROM {CATALOG}.silver.locations\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.gold.dim_locations\"):\n",
    "    source_df.write.saveAsTable(f\"{CATALOG}.gold.dim_locations\")\n",
    "    print(\"Created dim_locations\")\n",
    "else:\n",
    "    source_df.createOrReplaceTempView(\"source_locations\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG}.gold.dim_locations AS target\n",
    "        USING source_locations AS source\n",
    "        ON target.location_id = source.location_id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    print(\"Merged dim_locations\")\n",
    "\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {CATALOG}.gold.dim_locations\").collect()[0][0]\n",
    "print(f\"dim_locations: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f760b2b7-fa49-4c5b-9c7c-dde4880f4ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.0 - Dimension: Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de93e9a-8f9f-486e-9e52-3eea9b0764c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        parameter_id,\n",
    "        parameter_name,\n",
    "        parameter_units,\n",
    "        parameter_display_name\n",
    "    FROM {CATALOG}.silver.parameters\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.gold.dim_parameters\"):\n",
    "    source_df.write.saveAsTable(f\"{CATALOG}.gold.dim_parameters\")\n",
    "    print(\"Created dim_parameters\")\n",
    "else:\n",
    "    source_df.createOrReplaceTempView(\"source_parameters\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG}.gold.dim_parameters AS target\n",
    "        USING source_parameters AS source\n",
    "        ON target.parameter_id = source.parameter_id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    print(\"Merged dim_parameters\")\n",
    "\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {CATALOG}.gold.dim_parameters\").collect()[0][0]\n",
    "print(f\"dim_parameters: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ae2645-1b3b-4d37-b3fc-5c111a57b1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.0 - Dimension: Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a147366-11fe-49d9-953d-d265a40cf150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        sensor_id,\n",
    "        sensor_name\n",
    "    FROM {CATALOG}.silver.sensors\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.gold.dim_sensors\"):\n",
    "    source_df.write.saveAsTable(f\"{CATALOG}.gold.dim_sensors\")\n",
    "    print(\"Created dim_sensors\")\n",
    "else:\n",
    "    source_df.createOrReplaceTempView(\"source_sensors\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG}.gold.dim_sensors AS target\n",
    "        USING source_sensors AS source\n",
    "        ON target.sensor_id = source.sensor_id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    print(\"Merged dim_sensors\")\n",
    "\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {CATALOG}.gold.dim_sensors\").collect()[0][0]\n",
    "print(f\"dim_sensors: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7041d5d-b304-4426-aeda-1ea53f71118b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.0 - Dimension: Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4417f860-98f2-4c97-b16c-cce297081e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        CAST(DATE(datetime_utc) AS DATE) as date_id,\n",
    "        YEAR(datetime_utc) as year,\n",
    "        MONTH(datetime_utc) as month,\n",
    "        DAY(datetime_utc) as day,\n",
    "        DAYOFWEEK(datetime_utc) as day_of_week,\n",
    "        DAYNAME(datetime_utc) as day_name,\n",
    "        WEEKOFYEAR(datetime_utc) as week_of_year,\n",
    "        QUARTER(datetime_utc) as quarter\n",
    "    FROM {CATALOG}.silver.measurements\n",
    "    WHERE datetime_utc IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.gold.dim_date\"):\n",
    "    source_df.write.saveAsTable(f\"{CATALOG}.gold.dim_date\")\n",
    "    print(\"Created dim_date\")\n",
    "else:\n",
    "    source_df.createOrReplaceTempView(\"source_dates\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG}.gold.dim_date AS target\n",
    "        USING source_dates AS source\n",
    "        ON target.date_id = source.date_id\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    print(\"Merged dim_date\")\n",
    "\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {CATALOG}.gold.dim_date\").collect()[0][0]\n",
    "print(f\"dim_date: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a716276-17a4-4f65-a368-3b7b24f603d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.0 - Fact: Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbe9f5e-ed7f-4b62-8113-fcd024b01016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        m.sensors_id as sensor_id,\n",
    "        m.locations_id as location_id,\n",
    "        s.parameter_id,\n",
    "        CAST(DATE(m.datetime_utc) AS DATE) as date_id,\n",
    "        m.datetime_utc,\n",
    "        m.value,\n",
    "        m.ingested_at\n",
    "    FROM {CATALOG}.silver.measurements m\n",
    "    JOIN {CATALOG}.silver.sensors s ON m.sensors_id = s.sensor_id\n",
    "\"\"\")\n",
    "\n",
    "if not spark.catalog.tableExists(f\"{CATALOG}.gold.fact_measurements\"):\n",
    "    source_df.write.saveAsTable(f\"{CATALOG}.gold.fact_measurements\")\n",
    "    print(\"Created fact_measurements\")\n",
    "else:\n",
    "    source_df.createOrReplaceTempView(\"source_facts\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG}.gold.fact_measurements AS target\n",
    "        USING source_facts AS source\n",
    "        ON target.sensor_id = source.sensor_id \n",
    "           AND target.datetime_utc = source.datetime_utc\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    print(\"Merged fact_measurements\")\n",
    "\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {CATALOG}.gold.fact_measurements\").collect()[0][0]\n",
    "print(f\"fact_measurements: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ecda6b9-33ba-420f-8cb1-d0f3aafa5b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.0 - Verify Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a6b437-784b-4429-b408-c07205409f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"dim_locations samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.gold.dim_locations LIMIT 5\").display()\n",
    "\n",
    "print(\"dim_parameters samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.gold.dim_parameters LIMIT 5\").display()\n",
    "\n",
    "print(\"dim_sensors samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.gold.dim_sensors LIMIT 5\").display()\n",
    "\n",
    "print(\"dim_date samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.gold.dim_date LIMIT 5\").display()\n",
    "\n",
    "print(\"fact_measurements samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.gold.fact_measurements LIMIT 5\").display()\n",
    "\n",
    "print(\"Row Counts\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT 'dim_locations' as table_name, COUNT(*) as rows FROM {CATALOG}.gold.dim_locations\n",
    "    UNION ALL\n",
    "    SELECT 'dim_parameters', COUNT(*) FROM {CATALOG}.gold.dim_parameters\n",
    "    UNION ALL\n",
    "    SELECT 'dim_sensors', COUNT(*) FROM {CATALOG}.gold.dim_sensors\n",
    "    UNION ALL\n",
    "    SELECT 'dim_date', COUNT(*) FROM {CATALOG}.gold.dim_date\n",
    "    UNION ALL\n",
    "    SELECT 'fact_measurements', COUNT(*) FROM {CATALOG}.gold.fact_measurements\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6065742904089225,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_gold_star_schema",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
