{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c09a821-beba-4688-9840-35692d58a06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver - Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd53e33-4b52-4af7-8297-2b076b176bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.0 - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15696695-c84c-4d31-9a27-6eb7062f664a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7301dfc7-b6ea-4694-b84f-4ea382823264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.0 - Silver Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffd272e-5e24-402c-bcae-dc29cbd2db70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_locations_silver = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        name,\n",
    "        locality,\n",
    "        timezone,\n",
    "        isMobile,\n",
    "        isMonitor,\n",
    "        CAST(get_json_object(coordinates, '$.latitude') AS DOUBLE) as latitude,\n",
    "        CAST(get_json_object(coordinates, '$.longitude') AS DOUBLE) as longitude,\n",
    "        get_json_object(country, '$.code') as country_code,\n",
    "        get_json_object(country, '$.name') as country_name,\n",
    "        get_json_object(owner, '$.name') as owner_name,\n",
    "        get_json_object(provider, '$.name') as provider_name,\n",
    "        CAST(get_json_object(datetimeLast, '$.utc') AS TIMESTAMP) as last_update_utc,\n",
    "        ingested_at\n",
    "    FROM {CATALOG}.{SCHEMA}.locations\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY ingested_at DESC) = 1\n",
    "\"\"\")\n",
    "\n",
    "# Data quality: remove invalid coordinates\n",
    "df_locations_clean = df_locations_silver.filter(\n",
    "    (F.col(\"latitude\").isNotNull()) & \n",
    "    (F.col(\"longitude\").isNotNull()) &\n",
    "    (F.col(\"latitude\").between(-90, 90)) &\n",
    "    (F.col(\"longitude\").between(-180, 180))\n",
    ")\n",
    "\n",
    "df_locations_clean.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.silver.locations\")\n",
    "print(f\"Saved {df_locations_clean.count()} locations to silver (removed {df_locations_silver.count() - df_locations_clean.count()} invalid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b949a32e-bb62-4e27-aaa0-7d489d9c9438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.0 - Silver Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88d2066-6776-4104-9538-d336d3aca95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sensors_silver = spark.sql(f\"\"\"\n",
    "    WITH parsed_locations AS (\n",
    "        SELECT \n",
    "            id as location_id,\n",
    "            from_json(sensors, 'ARRAY<STRUCT<id: INT, name: STRING, parameter: STRUCT<id: INT, name: STRING, units: STRING, displayName: STRING>>>') as sensors_array,\n",
    "            ingested_at\n",
    "        FROM {CATALOG}.{SCHEMA}.locations\n",
    "        QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY ingested_at DESC) = 1\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        sensor.id as sensor_id,\n",
    "        sensor.name as sensor_name,\n",
    "        sensor.parameter.id as parameter_id,\n",
    "        location_id\n",
    "    FROM parsed_locations\n",
    "    LATERAL VIEW explode(sensors_array) AS sensor\n",
    "    WHERE sensor.id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "df_sensors_silver.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.silver.sensors\")\n",
    "print(f\"Saved {df_sensors_silver.count()} sensors to silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37667944-5d71-4a9f-b204-b600425cf735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.0 - Silver Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25b4d82-7336-4a8d-aed3-36b7dc61ff63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell: Silver Parameters (extract from bronze FIRST)\n",
    "df_parameters_silver = spark.sql(f\"\"\"\n",
    "    WITH parsed_locations AS (\n",
    "        SELECT \n",
    "            from_json(sensors, 'ARRAY<STRUCT<id: INT, name: STRING, parameter: STRUCT<id: INT, name: STRING, units: STRING, displayName: STRING>>>') as sensors_array\n",
    "        FROM {CATALOG}.{SCHEMA}.locations\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        sensor.parameter.id as parameter_id,\n",
    "        sensor.parameter.name as parameter_name,\n",
    "        sensor.parameter.units as parameter_units,\n",
    "        sensor.parameter.displayName as parameter_display_name\n",
    "    FROM parsed_locations\n",
    "    LATERAL VIEW explode(sensors_array) AS sensor\n",
    "    WHERE sensor.parameter.id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "df_parameters_silver.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.silver.parameters\")\n",
    "print(f\"Saved {df_parameters_silver.count()} parameters to silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39cfd5b-6ef9-48be-81e4-c7b06c06ab45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.0 - Silver Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6cbcae-99aa-4a54-89cc-39869c00bfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_measurements_silver = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        sensors_id,\n",
    "        locations_id,\n",
    "        value,\n",
    "        CAST(get_json_object(datetime, '$.utc') AS TIMESTAMP) as datetime_utc,\n",
    "        CAST(get_json_object(coordinates, '$.latitude') AS DOUBLE) as latitude,\n",
    "        CAST(get_json_object(coordinates, '$.longitude') AS DOUBLE) as longitude,\n",
    "        ingested_at\n",
    "    FROM {CATALOG}.{SCHEMA}.measurements\n",
    "\"\"\")\n",
    "\n",
    "# Data quality: remove null values and invalid readings\n",
    "df_measurements_clean = df_measurements_silver.filter(\n",
    "    (F.col(\"value\").isNotNull()) &\n",
    "    (F.col(\"datetime_utc\").isNotNull()) &\n",
    "    (F.col(\"sensors_id\").isNotNull()) &\n",
    "    (F.col(\"locations_id\").isNotNull()) &\n",
    "    (F.col(\"value\") >= 0)  # Air quality values should be positive\n",
    ")\n",
    "\n",
    "df_measurements_clean.write.mode(\"overwrite\").saveAsTable(f\"{CATALOG}.silver.measurements\")\n",
    "print(f\"Saved {df_measurements_clean.count()} measurements to silver (removed {df_measurements_silver.count() - df_measurements_clean.count()} invalid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c11f4f3-2249-4962-b6c5-509f740d38f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.0 - Verify Silver tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a12dde-f652-4658-8798-1ebe4def0f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Locations samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.silver.locations LIMIT 5\").display()\n",
    "\n",
    "print(\"Sensors samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.silver.sensors LIMIT 5\").display()\n",
    "\n",
    "print(\"Parameters samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.silver.parameters LIMIT 5\").display()\n",
    "\n",
    "print(\"Measurements samples\")\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.silver.measurements LIMIT 5\").display()\n",
    "\n",
    "print(\"Row Counts\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT 'locations' as table_name, COUNT(*) as rows FROM {CATALOG}.silver.locations\n",
    "    UNION ALL\n",
    "    SELECT 'parameters', COUNT(*) FROM {CATALOG}.silver.parameters\n",
    "    UNION ALL\n",
    "    SELECT 'sensors', COUNT(*) FROM {CATALOG}.silver.sensors\n",
    "    UNION ALL\n",
    "    SELECT 'measurements', COUNT(*) FROM {CATALOG}.silver.measurements\n",
    "\"\"\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
